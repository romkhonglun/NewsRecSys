%%writefile
train.py
import os
import torch
import pytorch_lightning as L
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary, TQDMProgressBar
from pytorch_lightning.loggers import WandbLogger
from dotenv import load_dotenv

# Import c√°c module ƒë√£ t·∫°o ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc
from dataset import NAMLDataModule
from variant_naml import VariantNAMLConfig
from time_feature_model import TIME_FEATURE_NAMLConfig
from lightning_module import NAMLLightningModule

# Load bi·∫øn m√¥i tr∆∞·ªùng (WANDB_API_KEY, etc.)
load_dotenv()

# ==========================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (QUAN TR·ªåNG)
# ==========================================
# 1. N∆°i ch·ª©a data ƒë√£ ch·∫°y qua preprocess.py (quan tr·ªçng nh·∫•t)
# L∆∞u √Ω: preprocess.py l∆∞u v√†o /kaggle/working/processed
PROCESSED_DIR = "/kaggle/working/processed"

# 2. ƒê∆∞·ªùng d·∫´n ƒë·∫øn file vector embedding (.npy)
# N·∫øu b·∫°n ch∆∞a c√≥ file n√†y, h√£y t·∫°o dummy ho·∫∑c tr·ªè t·∫°m v√†o ƒë√¢u ƒë√≥.
# Model s·∫Ω t·ª± t·∫°o random n·∫øu kh√¥ng t√¨m th·∫•y file n√†y (nh∆∞ logic trong lightning_module.py)
EMBEDDING_PATH = "/kaggle/working/processed_data/body_emb.npy"


def main():
    L.seed_everything(42)  # Set seed ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£

    # 1. Init Config
    print("Initializing Configuration...")
    config = TIME_FEATURE_NAMLConfig()

    # In th√¥ng s·ªë ki·ªÉm tra
    print(f"Model Config: Window={config.window_size}, Interests={config.num_interests}")

    # 2. Init DataModule
    # L∆∞u √Ω: Class n√†y gi·ªù nh·∫≠n 'processed_dir' ch·ª© kh√¥ng ph·∫£i 'root_path'
    dm = NAMLDataModule(
        processed_dir=PROCESSED_DIR,
        embedding_path=EMBEDDING_PATH,
        batch_size=512,  # TƒÉng l√™n n·∫øu VRAM c√≤n tr·ªëng (512 l√† an to√†n cho T4 x2)
        history_len=30,
        num_workers=2  # Kaggle c√≥ 2 core CPU m·∫°nh ho·∫∑c 4 core y·∫øu, ƒë·ªÉ 2 l√† an to√†n
    )

    # 3. Init Model (Lightning Module)
    # L∆∞u √Ω: B·ªè tham s·ªë 'mode' v√¨ code m·ªõi ch·ªâ ch·∫°y VariantNAML
    model = NAMLLightningModule(
        config=config,
        embedding_path=EMBEDDING_PATH,
        lr=1e-3,
        weight_decay=1e-5
    )

    # 4. Logger (Wandb)
    # Set log_model=False ƒë·ªÉ ƒë·ª° t·ªën dung l∆∞·ª£ng upload model l√™n m√¢y
    wandb_logger = WandbLogger(
        project="NAML-News-Rec",
        name="Variant-NAML-Final",
        log_model=False
    )

    # 5. Callbacks
    checkpoint_callback = ModelCheckpoint(
        dirpath="checkpoints",
        filename="naml-{epoch:02d}-{val/auc:.4f}",
        save_top_k=20,
        monitor="val/auc",
        mode="max",
        verbose=True
    )

    early_stop_callback = EarlyStopping(
        monitor="val/auc",
        min_delta=0.0001,
        patience=5,  # Gi·∫£m patience xu·ªëng 3 ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian GPU Kaggle
        verbose=True,
        mode="max"
    )

    # 6. Trainer
    trainer = L.Trainer(
        accelerator="auto",
        devices="auto",
        strategy="auto",
        logger=wandb_logger,
        callbacks=[
            checkpoint_callback,
            early_stop_callback,
            ModelSummary(max_depth=2),
            TQDMProgressBar(refresh_rate=10)
        ],
        gradient_clip_algorithm="norm",
        max_epochs=20,  # Train nhi·ªÅu epoch h∆°n (Early Stop s·∫Ω lo ph·∫ßn d·ª´ng)
        # log_every_n_steps=50,

        # [QUAN TR·ªåNG] Precision 16-mixed gi√∫p gi·∫£m 1/2 VRAM v√† train nhanh g·∫•p ƒë√¥i tr√™n T4
        precision="32",

        # C·∫Øt gradient ƒë·ªÉ ·ªïn ƒë·ªãnh Transformer training
        # gradient_clip_val=0.5,

        # Ki·ªÉm tra validation loop tr∆∞·ªõc khi train ƒë·ªÉ ƒë·∫£m b·∫£o code kh√¥ng bug
        # num_sanity_val_steps=2
    )

    print("üöÄ Starting training...")
    trainer.fit(model, datamodule=dm)

    print(f"‚úÖ Training finished. Best model path: {checkpoint_callback.best_model_path}")


if __name__ == "__main__":
    main()